{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901f4e95",
   "metadata": {},
   "source": [
    "### Imports and basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd089bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, unquote, quote\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "URL_INFOS = \"https://{lang}.wikipedia.org/w/api.php\"\n",
    "URL_STATS = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{lang}.wikipedia/{access}/{agent}/{uri_article_name}/{granularity}/{start}/{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"EPFL WikiStats\",\n",
    "    \"From\": \"noreply@epfl.ch\",\n",
    "    \"Accept\": \"json\",\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "GLOBAL_LIMIT = 25\n",
    "WIKI_LIMIT = 500  # From the API\n",
    "\n",
    "BACKLINKS_LIMIT = GLOBAL_LIMIT\n",
    "CONTRIBS_LIMIT = WIKI_LIMIT\n",
    "DEFAULT_DURATION = int(2 * 365.25)\n",
    "ACCESS = \"all-access\"\n",
    "AGENTS = \"all-agents\"\n",
    "GRANULARITY = \"daily\"\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e655a5",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lang_name(link: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract name and lang\n",
    "    \"\"\"\n",
    "    if \"//\" not in link:\n",
    "        link = f\"//{link}\"\n",
    "    parsed = urlparse(link)\n",
    "    lang = parsed.hostname.split(\".\")[0]\n",
    "    name = parsed.path.split(\"/\")[-1]\n",
    "    return lang, unquote(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8157dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qprint(json_queries):\n",
    "    \"\"\"\n",
    "    Check if correct JSON and prints.\n",
    "    \"\"\"\n",
    "    print(json.dumps(json_queries, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbaa280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_quote(page_name):\n",
    "    \"\"\"\n",
    "    Transform into valid wiki URI.\n",
    "    \"\"\"\n",
    "    return quote(page_name.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f99458",
   "metadata": {},
   "source": [
    "### Inputs from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links are provided\n",
    "target_links = [\n",
    "    \"https://fr.wikipedia.org/wiki/Martin_Vetterli\",\n",
    "    \"https://en.wikipedia.org/wiki/%C3%89cole_Polytechnique_F%C3%A9d%C3%A9rale_de_Lausanne\",\n",
    "    \"https://en.wikipedia.org/wiki/Jean-Pierre_Hubaux\",\n",
    "    \"https://it.wikipedia.org/wiki/Jean-Michel_LErreur\",\n",
    "    \"https://fr.wikipedia.org/wiki/%C3%89cole_polytechnique_f%C3%A9d%C3%A9rale_de_Lausanne\",\n",
    "    \"it.wikipedia.org/wiki/Jean-Michel_LErreur\",\n",
    "    \"Michael_Gr√§tzel\",\n",
    "    \"Patrick Aebischer\",\n",
    "]\n",
    "\n",
    "# Chosen Langs\n",
    "target_langs = [\"en\", \"fr\", \"de\"]\n",
    "\n",
    "# Chosen contributors\n",
    "target_contributors = []\n",
    "\n",
    "# Length for the revisions, in days\n",
    "target_duration = DEFAULT_DURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f5157",
   "metadata": {},
   "source": [
    "### Aimed data format\n",
    "\n",
    "The final result will look like this.\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"Complete name\": {\n",
    "        \"error\": \"A description of the error.\",  # Facultative. If an error occured, the data is considered not to be used.\n",
    "        \"query\": {  # Informations about the base query\n",
    "            \"lang\": \"lang_code\",\n",
    "            \"pid\": 999,  # Facultative in case of error\n",
    "            \"timestamp\": \"YYYY-MM-DDTHH:mm:ssZ\",  # ISO 8601\n",
    "            \"duration\": 999,  # Length for the revisions, in days\n",
    "        },\n",
    "        \"langs\": {\n",
    "            \"lang_code\": {\n",
    "                \"pid\": 999,\n",
    "                \"pwikidata\": \"Q888\",  # Wikidata ID\n",
    "                \"name\": \"Complete name (in that language)\",\n",
    "                \"creation\": {\n",
    "                    \"timestamp\": \"YYYY-MM-DDTHH:mm:ssZ\",  # ISO 8601\n",
    "                    \"username\": \"someone\",\n",
    "                },\n",
    "                \"last_edit\": {\n",
    "                    \"timestamp\": \"YYYY-MM-DDTHH:mm:ssZ\",  # ISO 8601\n",
    "                    \"username\": \"someone\",\n",
    "                },\n",
    "                \"backlinks\": [  # Facultative. List of pages.\n",
    "                    \"Another page\",\n",
    "                    ...\n",
    "                ],\n",
    "                \"contributors\": [  # List of filtered contributors to the page\n",
    "                    \"Jean-Michel\",\n",
    "                    ...\n",
    "                ],\n",
    "                \"contributions\": {\n",
    "                    \"items\": [  # List of filtered contributions to the page\n",
    "                        {\n",
    "                            \"timestamp\": \"YYYY-MM-DDTHH:mm:ssZ\",  # ISO 8601\n",
    "                            \"username\": \"someone\",\n",
    "                            \"size\": 999,\n",
    "                        },\n",
    "                        ...\n",
    "                    ],\n",
    "                },\n",
    "                \"pageviews\": {\n",
    "                    \"granularity\": \"daily\",  # time unit, e.g., daily or monthly\n",
    "                    \"access\": \"all-access\",  # access method, e.g., all-access\n",
    "                    \"agent\": \"all-agents\",  # agent type, e.g., user\n",
    "                    \"items\": [  # List of pageviews\n",
    "                        {\n",
    "                            \"timestamp\": \"YYYY-MM-DDTHH:mm:ssZ\",  # ISO 8601\n",
    "                            \"views\": 42,\n",
    "                        },\n",
    "                        ...\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "            ...\n",
    "        },\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17ff5b",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd507f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find = {}\n",
    "for link in target_links:\n",
    "    # If it's a link, extract lang and name\n",
    "    if \"wikipedia.org\" in link:\n",
    "        lang, name = extract_lang_name(link)\n",
    "        if lang not in to_find:\n",
    "            to_find[lang] = set()\n",
    "        to_find[lang].add(name)\n",
    "    # Else, we assume it's directly a name, and try to find it\n",
    "    else:\n",
    "        if \"*\" not in to_find:\n",
    "            to_find[\"*\"] = set()\n",
    "        to_find[\"*\"].add(link)\n",
    "\n",
    "# Each name without a lang will be tracked down using target langs\n",
    "for name in to_find[\"*\"]:\n",
    "    for lang in target_langs:\n",
    "        if lang not in to_find:\n",
    "            to_find[lang] = set()\n",
    "        to_find[lang].add(name)\n",
    "        \n",
    "del to_find[\"*\"]\n",
    "        \n",
    "if VERBOSE:\n",
    "    pprint(to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9949b33",
   "metadata": {},
   "source": [
    "### Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfff9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts request session, that will be used through the whole process\n",
    "s = requests.Session()\n",
    "s.headers.update(HEADERS)\n",
    "s.params.update(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the page exists, gather informations if it does\n",
    "# https://www.mediawiki.org/wiki/API:Info\n",
    "# https://www.mediawiki.org/wiki/API:Langlinks\n",
    "queries = {}\n",
    "for lang, names in to_find.items():\n",
    "    url_full = URL_INFOS.format(lang=lang)\n",
    "    # We group the queries per target lang for less queries\n",
    "    titles = \"|\".join(names)\n",
    "    params = {\n",
    "        \"titles\": titles,\n",
    "        \"prop\": \"info|langlinks\",\n",
    "        \"lllimit\": WIKI_LIMIT,  # We want all langs in order to find our target langs\n",
    "    }\n",
    "    \n",
    "    results = s.get(url=url_full, params=params)\n",
    "    data = results.json()\n",
    "\n",
    "    if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "        data = data[\"query\"][\"pages\"]\n",
    "    \n",
    "    for pid, obj in data.items():\n",
    "        title = obj[\"title\"]\n",
    "        \n",
    "        # Will only keep the latest successful query for same name pages\n",
    "        queries[title] = {\n",
    "            \"query\": {\n",
    "                \"lang\": lang,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Page was not found with that language\n",
    "        if int(pid) < 0:\n",
    "            queries[title][\"error\"] = \"not found\"\n",
    "            continue\n",
    "        \n",
    "        queries[title][\"query\"].update({\n",
    "            \"pid\": int(pid),\n",
    "            \"timestamp\": datetime.datetime.today().isoformat(),\n",
    "            \"duration\": target_duration,\n",
    "        })\n",
    "        \n",
    "        # Add the query language in the list of langs\n",
    "        queries[title][\"langs\"] = {\n",
    "            lang: {\n",
    "                \"name\": title,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add the other target langs\n",
    "        if \"langlinks\" in obj:\n",
    "            for langlink in obj[\"langlinks\"]:\n",
    "                if not target_langs or langlink[\"lang\"] in target_langs:  # Use all langs if no target lang are specified\n",
    "                    queries[title][\"langs\"][langlink[\"lang\"]] = {\n",
    "                        \"name\": langlink[\"*\"]\n",
    "                    }\n",
    "        \n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge linked pages with different names\n",
    "# We assume here that pages are correctly linked (by Wikipedia) between each other\n",
    "next_queries = {}\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        next_queries[name] = obj\n",
    "        continue\n",
    "        \n",
    "    skip = False\n",
    "    for _, page in obj[\"langs\"].items():\n",
    "        if page[\"name\"] in next_queries.keys():\n",
    "            skip = True\n",
    "            break\n",
    "            \n",
    "    if not skip:\n",
    "        next_queries[name] = obj\n",
    "queries = next_queries\n",
    "        \n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b747f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the backlinks for each\n",
    "# For important pages (looking at you, \"√âcole polytechnique f√©d√©rale de Lausanne\"), can take some time!\n",
    "# Set BACKLINKS_LIMIT to control that.\n",
    "# https://www.mediawiki.org/wiki/API:Backlinks\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        continue\n",
    "        \n",
    "    for lang, page in obj[\"langs\"].items():\n",
    "        blcontinue = \"\"\n",
    "        blcounter = 0\n",
    "        url_full = URL_INFOS.format(lang=lang)\n",
    "        \n",
    "        while blcounter < BACKLINKS_LIMIT:\n",
    "            params = {\n",
    "                \"list\": \"backlinks\",\n",
    "                \"bltitle\": page[\"name\"],\n",
    "                \"bllimit\": min(BACKLINKS_LIMIT, WIKI_LIMIT),\n",
    "            }\n",
    "            if blcontinue != \"\":\n",
    "                params[\"blcontinue\"] = blcontinue\n",
    "                \n",
    "            results = s.get(url=url_full, params=params)\n",
    "            data = results.json()\n",
    "            \n",
    "            if \"query\" in data and \"backlinks\" in data[\"query\"]:\n",
    "                bldata = data[\"query\"][\"backlinks\"]\n",
    "            else:\n",
    "                obj[\"error\"] = \"could not retrieve informations (backlinks)\"\n",
    "                break\n",
    "                \n",
    "            if \"backlinks\" not in page:\n",
    "                page[\"backlinks\"] = set()  # This is to delete doubles\n",
    "                \n",
    "            if bldata:\n",
    "                for backlink in bldata:\n",
    "                    page[\"backlinks\"].add(backlink[\"title\"])\n",
    "                    blcounter += 1\n",
    "                    \n",
    "            if \"continue\" in data:\n",
    "                blcontinue = data[\"continue\"][\"blcontinue\"]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if \"backlinks\" in page and isinstance(page[\"backlinks\"], set):\n",
    "            page[\"backlinks\"] = list(page[\"backlinks\"])  # Sets are not valid JSON objects, lists are\n",
    "\n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some of the missing informations\n",
    "# https://www.mediawiki.org/wiki/API:Pageprops\n",
    "# https://www.mediawiki.org/wiki/API:Revisions\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        continue\n",
    "        \n",
    "    for lang, page in obj[\"langs\"].items():\n",
    "        url_full = URL_INFOS.format(lang=lang)\n",
    "        params = {\n",
    "            \"titles\": page[\"name\"],\n",
    "            \"prop\": \"pageprops|revisions\",\n",
    "            \"rvlimit\": 1,\n",
    "            \"rvprop\": \"timestamp|user\",\n",
    "            \"rvdir\": \"newer\",\n",
    "        }\n",
    "        \n",
    "        results = s.get(url=url_full, params=params)\n",
    "        data = results.json()\n",
    "        \n",
    "        if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "            content = data[\"query\"][\"pages\"]\n",
    "            pid = next(iter(content))\n",
    "            page[\"pid\"] = int(pid)\n",
    "            content = content[pid]\n",
    "            page[\"pwikidata\"] = content[\"pageprops\"][\"wikibase_item\"]\n",
    "            page[\"creation\"] = {\n",
    "                \"timestamp\": content[\"revisions\"][0][\"timestamp\"],\n",
    "                \"user\": content[\"revisions\"][0][\"user\"],\n",
    "            }\n",
    "        else:\n",
    "            obj[\"error\"] = \"could not retrieve informations (props)\"\n",
    "        \n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d596098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contributors\n",
    "# https://www.mediawiki.org/wiki/API:Contributors\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        continue\n",
    "        \n",
    "    for lang, page in obj[\"langs\"].items():\n",
    "        pccontinue = \"\"\n",
    "        pccounter = 0\n",
    "        url_full = URL_INFOS.format(lang=lang)\n",
    "        params = {\n",
    "            \"titles\": page[\"name\"],\n",
    "            \"prop\": \"contributors\",\n",
    "            \"pclimit\": min(CONTRIBS_LIMIT, WIKI_LIMIT),\n",
    "        }\n",
    "        \n",
    "        while pccounter < CONTRIBS_LIMIT:\n",
    "            if pccontinue != \"\":\n",
    "                params[\"pccontinue\"] = pccontinue\n",
    "                \n",
    "            results = s.get(url=url_full, params=params)\n",
    "            data = results.json()\n",
    "            \n",
    "            if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "                pcdata = data[\"query\"][\"pages\"][str(page[\"pid\"])]\n",
    "            else:\n",
    "                obj[\"error\"] = \"could not retrieve informations (contributors)\"\n",
    "                break\n",
    "                \n",
    "            if \"contributors\" not in page:\n",
    "                page[\"contributors\"] = set()  # Data should already be a set, but I'm being cautious\n",
    "                \n",
    "            if pcdata:\n",
    "                page[\"contributors\"].update(\n",
    "                    [\n",
    "                        contributor[\"name\"]\n",
    "                        for contributor in pcdata[\"contributors\"]\n",
    "                        if not target_contributors\n",
    "                        or contributor[\"name\"] in target_contributors  # Use all contributors if no target contributors are specified\n",
    "                    ]\n",
    "                )\n",
    "                pccounter += len(pcdata[\"contributors\"])\n",
    "                \n",
    "            if \"continue\" in data:\n",
    "                pccontinue = data[\"continue\"][\"pccontinue\"]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if \"contributors\" in page and isinstance(page[\"contributors\"], set):\n",
    "            page[\"contributors\"] = list(page[\"contributors\"])  # Sets are not valid JSON objects, lists are\n",
    "        \n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c896f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Contributions\n",
    "# https://www.mediawiki.org/wiki/API:Revisions\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        continue\n",
    "        \n",
    "    for lang, page in obj[\"langs\"].items():\n",
    "        rvcontinue = \"\"\n",
    "        url_full = URL_INFOS.format(lang=lang)\n",
    "        params = {\n",
    "            \"titles\": page[\"name\"],\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvprop\": \"timestamp|user|size\",\n",
    "            \"rvstart\": obj[\"query\"][\"timestamp\"],\n",
    "            \"rvend\": (datetime.datetime.fromisoformat(obj[\"query\"][\"timestamp\"]) - datetime.timedelta(days=obj[\"query\"][\"duration\"])).isoformat(),\n",
    "            \"rvdir\": \"older\",  # rvstart has to be later than rvend with that mode\n",
    "            \"rvlimit\": WIKI_LIMIT,\n",
    "        }\n",
    "        \n",
    "        while True:\n",
    "            if rvcontinue != \"\":\n",
    "                params[\"rvcontinue\"] = rvcontinue\n",
    "                \n",
    "            results = s.get(url=url_full, params=params)\n",
    "            data = results.json()\n",
    "            \n",
    "            if \"query\" in data and \"pages\" in data[\"query\"] and \"revisions\" in data[\"query\"][\"pages\"][str(page[\"pid\"])]:\n",
    "                rvdata = data[\"query\"][\"pages\"][str(page[\"pid\"])][\"revisions\"]\n",
    "            else:\n",
    "                obj[\"error\"] = \"could not retrieve informations (contributions)\"\n",
    "                break\n",
    "                \n",
    "            if \"contributions\" not in page:\n",
    "                page[\"contributions\"] = {\n",
    "                    \"items\": [],\n",
    "                }\n",
    "                \n",
    "            if rvdata:\n",
    "                for revision in rvdata:\n",
    "                    page[\"contributions\"][\"items\"].append({\n",
    "                        \"timestamp\": revision[\"timestamp\"],\n",
    "                        \"username\": revision[\"user\"],\n",
    "                        \"size\": revision[\"size\"],\n",
    "                    })\n",
    "                    \n",
    "            if \"continue\" in data:\n",
    "                rvcontinue = data[\"continue\"][\"rvcontinue\"]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "if VERBOSE:\n",
    "    qprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757c762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pageviews\n",
    "# https://wikimedia.org/api/rest_v1/#/Pageviews%20data/get_metrics_pageviews_per_article__project___access___agent___article___granularity___start___end_\n",
    "for name, obj in queries.items():\n",
    "    if \"error\" in obj:\n",
    "        continue\n",
    "        \n",
    "    for lang, page in obj[\"langs\"].items():\n",
    "        url_full = URL_STATS.format(\n",
    "            lang=lang,\n",
    "            access=ACCESS,\n",
    "            agent=AGENTS,\n",
    "            uri_article_name=wiki_quote(page[\"name\"]),\n",
    "            granularity=GRANULARITY,\n",
    "            start=(datetime.datetime.fromisoformat(obj[\"query\"][\"timestamp\"]) - datetime.timedelta(days=obj[\"query\"][\"duration\"])).strftime('%Y%m%d00'),\n",
    "            end=datetime.datetime.fromisoformat(obj[\"query\"][\"timestamp\"]).strftime('%Y%m%d00'),\n",
    "        )\n",
    "        \n",
    "        results = s.get(url=url_full)\n",
    "        data = results.json()\n",
    "        \n",
    "        if \"items\" in data:\n",
    "            if \"pageviews\" not in page:\n",
    "                page[\"pageviews\"] = {\n",
    "                    \"granularity\": GRANULARITY,\n",
    "                    \"access\": ACCESS,\n",
    "                    \"agent\": AGENTS,\n",
    "                    \"items\": [],\n",
    "                }\n",
    "                \n",
    "            for item in data[\"items\"]:\n",
    "                page[\"pageviews\"][\"items\"].append({\n",
    "                    \"timestamp\": datetime.datetime.strptime(item[\"timestamp\"], \"%Y%m%d%H\").isoformat(),\n",
    "                    \"views\": item[\"views\"],\n",
    "                })\n",
    "        else:\n",
    "            obj[\"error\"] = \"could not retrieve informations (pageviews)\"\n",
    "            continue\n",
    "\n",
    "if VERBOSE:\n",
    "    qprint(queries)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a170cf",
   "metadata": {},
   "source": [
    "### Final result for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f91d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = \"Jean-Pierre Hubaux\"\n",
    "print(f\"#### {example}\")\n",
    "qprint(queries[example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d562f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
